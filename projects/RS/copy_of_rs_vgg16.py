# -*- coding: utf-8 -*-
"""Copy of RS-VGG16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sJh8Lk_D_rv0QsI2bn6GI_cPCwl7DREN

# install data
"""

import os
import json
import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.image import img_to_array, load_img
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical

# install Kaggle
!pip install -q kaggle

from google.colab import files
files.upload()

#Creat a kaggle folder
!mkdir ~/.kaggle

#copy the kaggle.json to folder created
!cp kaggle.json  ~/.kaggle/

#permission for the json to act
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d paramaggarwal/fashion-product-images-dataset

!unzip fashion-product-images-dataset.zip

"""# EDA"""

!pip install keras-tuner
!pip install wandb

# Commented out IPython magic to ensure Python compatibility.
#ignore warnings
import warnings
warnings.filterwarnings("ignore")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import cv2
import random

import tensorflow as tf
# data generator
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# wandb
import wandb
from wandb.keras import WandbCallback

DATA_PATH = './fashion-dataset'

# %matplotlib inline

images_df = pd.read_csv(
    os.path.join(DATA_PATH, 'fashion-dataset', 'images.csv')
)

images_df.head(2)

styles_df = pd.read_csv(
    os.path.join(DATA_PATH, 'fashion-dataset', 'styles.csv'),
    on_bad_lines='skip'
)

styles_df.head(2)

images_df.head(2)

images_df['id'] = images_df['filename']\
.apply(lambda filename: filename\
       .replace('.jpg', '')).astype(int)

images_df.head(2)

data = styles_df.merge(images_df, on='id', how='left')
data.head(2)

# convert filename to filepath
data['filename'] = data['filename']\
.apply(lambda filename: os.path.join(DATA_PATH, 'fashion-dataset', 'images', filename))
data.head(2)

# get list of images in dataset
image_files = os.listdir(
    os.path.join(DATA_PATH, 'fashion-dataset', 'images')
)
print(len(image_files))

# files included in the dataset
data['file_found'] = data['id'].apply(lambda idx: '{}.jpg'.format(idx)  in image_files)

data['file_found'].value_counts()

# delete files without images in the dataset
data = data[data['file_found']].reset_index(drop=True)
print(data.shape)
data.head(2)

data.isnull().sum()

# trực quan một số hình ảnh trong datasets
def dislay_image(image_files):
    random.shuffle(image_files)
    for idx, image_file in enumerate(image_files[0:9]):
        plt.subplot(3, 3, idx+1)
        image_path = os.path.join(DATA_PATH, 'fashion-dataset', 'images', image_file)
        image_arr = cv2.imread(image_path)
        image_arr = cv2.cvtColor(image_arr, cv2.COLOR_BGR2RGB)

        plt.imshow(image_arr)
        plt.axis("off")

dislay_image(image_files)

# masterCategory count
gr_data_masterCate = data.groupby('masterCategory').size()
gr_data_masterCate_sorted= gr_data_masterCate.sort_values()
gr_data_masterCate_sorted

# subCategory count
gr_data_subCate = data.groupby('subCategory').size()
gr_data_subCate_sorted= gr_data_subCate.sort_values()

len(gr_data_subCate_sorted)

# articleType count
gr_data_articleType = data.groupby('articleType').size()
gr_data_articleType_sorted = gr_data_articleType.sort_values()

categoricals = sorted(list(gr_data_subCate_sorted.index[-20:]))

data_20 = data[data['subCategory'].isin(categoricals)]

data_20 = data_20[['subCategory', 'filename']]

data_20

data_20.groupby('subCategory').size().sort_values(ascending=False)

from sklearn.utils import resample, shuffle
from sklearn.model_selection import train_test_split

n_samples = 600
lst_df = []
for categorical in categoricals:
    df_class_tmp = data_20.loc[data_20['subCategory'] == categorical]
    if df_class_tmp.shape[0] < n_samples:
        df_resample_tmp = df_class_tmp
    else:
        df_resample_tmp = resample(df_class_tmp, n_samples=n_samples, random_state=42)
    lst_df.append(df_resample_tmp)
df = pd.concat(lst_df)

df.shape

df = shuffle(df, random_state=42)
df = df.reset_index(drop=True)

df.rename({'subCategory':'categorical'}, axis=1, inplace=True)

# final data
data = df
data

"""# Splite data"""

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(data,\
                                     test_size=0.2,\
                                     random_state=42,\
                                     stratify=data['categorical'])
valid_df, test_df = train_test_split(test_df,\
                                      test_size=0.5,\
                                      random_state=42,\
                                      stratify=test_df['categorical'])

train_df = train_df.reset_index(drop=True)
valid_df = valid_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

train_df

"""# Data augmentation"""

datagen = ImageDataGenerator(rescale=1/255.,
                            shear_range=0.2,
                            zoom_range=0.2,
                            horizontal_flip=True)

train_generator = datagen.flow_from_dataframe(dataframe=train_df,
                                             target_size=(224,224),
                                             x_col='filename',
                                            y_col='categorical',
                                             class_mode='categorical',
                                             batch_size=32,
                                             shuffle=True,
                                              seed=42)


test_datagen = ImageDataGenerator(rescale=1/255.)
valid_generator = test_datagen.flow_from_dataframe(dataframe=valid_df,
                                             target_size=(224,224),
                                             x_col='filename',
                                            y_col='categorical',
                                             class_mode='categorical',
                                             batch_size=32,
                                             shuffle=True,
                                              seed=42)

test_generator = test_datagen.flow_from_dataframe(
    dataframe=test_df,
    x_col='filename',
    y_col='categorical',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=True,
    seed=42
)

"""# Build And Train the Model

# VGG16
"""

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

def build_model_VGG16(name, weights_path=None):
    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    for layer in base_model.layers:
        layer.trainable = False

    x = base_model.output
    x = Flatten()(x)
    x = Dense(4096, activation='leaky_relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    x = Dense(1024, activation='sigmoid')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    predictions = Dense(20, activation='softmax')(x)

    model = Model(name=name, inputs=base_model.input, outputs=predictions)
    if weights_path:
        model.load_weights(weights_path)
    return model

import time

NAME = "vgg16-{}".format(int(time.time()))
model_VGG16 = build_model_VGG16(NAME)
model_VGG16.summary()

lr = 0.01
epochs = 10

model_VGG16.compile(loss='categorical_crossentropy',
              optimizer=Adam(learning_rate=lr),
              metrics=['accuracy'])

history_VGG16 = model_VGG16.fit_generator(train_generator,
                    validation_data = train_generator,
                    steps_per_epoch = train_generator.n//train_generator.batch_size,
                    validation_steps = valid_generator.n//valid_generator.batch_size,
                    epochs=epochs,
                    )

model_VGG16.save('VGG16_model.h5')

# Plot losses
plt.plot(history_VGG16.history['loss'], label="train loss")
plt.plot(history_VGG16.history['val_loss'], label="test loss")
plt.legend()
plt.show()

# Plot MSE
plt.plot(history_VGG16.history['accuracy'], label="train mse")
plt.plot(history_VGG16.history[ 'val_accuracy'], label="test mse")
plt.legend()
plt.show()

score_VGG16 = model_VGG16.evaluate_generator(test_generator)
print('Test loss:', score_VGG16[0])
print('Test accuracy:', score_VGG16[1])

#from tensorflow.keras.models import load_model
#best_model = load_model('VGG16_model.h5')

#score = best_model.evaluate_generator(test_generator)
#print('Test loss:', score[0])
#print('Test accuracy:', score[1])

"""# VGG19"""

from tensorflow.keras.applications import VGG19

def build_model_VGG19(name, weights_path=None):
    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    for layer in base_model.layers:
        layer.trainable = False

    x = base_model.output
    x = Flatten()(x)
    x = Dense(4096, activation='leaky_relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    x = Dense(1024, activation='sigmoid')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    predictions = Dense(20, activation='softmax')(x)

    model = Model(name=name, inputs=base_model.input, outputs=predictions)
    if weights_path:
        model.load_weights(weights_path)
    return model

NAME = "InceptionV3-{}".format(int(time.time()))
model_VGG19 = build_model_VGG19(NAME)
model_VGG19.summary()

model_VGG19.compile(loss='categorical_crossentropy',
              optimizer=Adam(learning_rate=lr),
              metrics=['accuracy'])

history_VGG19 = model_VGG19.fit_generator(train_generator,
                    validation_data = train_generator,
                    steps_per_epoch = train_generator.n//train_generator.batch_size,
                    validation_steps = valid_generator.n//valid_generator.batch_size,
                    epochs=epochs,
                    )

model_VGG19.save('VGG19_model.h5')

# Plot losses
plt.plot(history_VGG19.history['loss'], label="train loss")
plt.plot(history_VGG19.history['val_loss'], label="test loss")
plt.legend()
plt.show()

# Plot MSE
plt.plot(history_VGG19.history['accuracy'], label="train mse")
plt.plot(history_VGG19.history[ 'val_accuracy'], label="test mse")
plt.legend()
plt.show()

score_VGG19 = model_VGG19.evaluate_generator(test_generator)
print('Test loss:', score_VGG19[0])
print('Test accuracy:', score_VGG19[1])

"""# Inception"""

def build_model_Inception(name, weights_path=None):
    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    for layer in base_model.layers:
        layer.trainable = False

    x = base_model.output
    x = Flatten()(x)
    x = Dense(4096, activation='leaky_relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    x = Dense(1024, activation='sigmoid')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    predictions = Dense(20, activation='softmax')(x)

    model = Model(name=name, inputs=base_model.input, outputs=predictions)
    if weights_path:
        model.load_weights(weights_path)
    return model

NAME = "InceptionV3-{}".format(int(time.time()))
model_InceptionV3 = build_model_Inception(NAME)
model_InceptionV3.summary()

model_InceptionV3.compile(loss='categorical_crossentropy',
              optimizer=Adam(learning_rate=lr),
              metrics=['accuracy'])

history_InceptionV3 = model_InceptionV3.fit_generator(train_generator,
                    validation_data = train_generator,
                    steps_per_epoch = train_generator.n//train_generator.batch_size,
                    validation_steps = valid_generator.n//valid_generator.batch_size,
                    epochs=epochs,
                    )

model_InceptionV3.save('Inceptionv3_model.h5')

# Plot losses
plt.plot(history_InceptionV3.history['loss'], label="train loss")
plt.plot(history_InceptionV3.history['val_loss'], label="test loss")
plt.legend()
plt.show()

# Plot MSE
plt.plot(history_InceptionV3.history['accuracy'], label="train mse")
plt.plot(history_InceptionV3.history[ 'val_accuracy'], label="test mse")
plt.legend()
plt.show()

score_InceptioV3 = model_InceptionV3.evaluate_generator(test_generator)
print('Test loss:', score_InceptioV3[0])
print('Test accuracy:', score_InceptioV3[1])

"""# ResNet"""

from tensorflow.keras.applications import ResNet50

def build_model_ResNet50(name, weights_path=None):
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    for layer in base_model.layers:
        layer.trainable = False

    x = base_model.output
    x = Flatten()(x)
    x = Dense(4096, activation='leaky_relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    x = Dense(1024, activation='sigmoid')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    predictions = Dense(20, activation='softmax')(x)

    model = Model(name=name, inputs=base_model.input, outputs=predictions)
    if weights_path:
        model.load_weights(weights_path)
    return model

import time
NAME = "ResNet50-{}".format(int(time.time()))
model_ResNet50 = build_model_ResNet50(NAME)
model_ResNet50.summary()

lr = 0.01
epochs = 10

model_ResNet50.compile(loss='categorical_crossentropy',
              optimizer=Adam(learning_rate=lr),
              metrics=['accuracy'])

history_ResNet50 = model_ResNet50.fit_generator(train_generator,
                    validation_data = train_generator,
                    steps_per_epoch = train_generator.n//train_generator.batch_size,
                    validation_steps = valid_generator.n//valid_generator.batch_size,
                    epochs=epochs,
                    )

model_ResNet50.save('ResNet50_model.h5')

# Plot losses
plt.plot(model_ResNet50.history['loss'], label="train loss")
plt.plot(model_ResNet50.history['val_loss'], label="test loss")
plt.legend()
plt.show()

# Plot MSE
plt.plot(model_ResNet50.history['accuracy'], label="train mse")
plt.plot(model_ResNet50.history[ 'val_accuracy'], label="test mse")
plt.legend()
plt.show()

print(test_generator)

test_df.head()

test_generator_image = test_datagen.flow_from_dataframe(
    dataframe=test_df[0],
    x_col='filename',
    y_col='categorical',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=True,
    seed=42
)

score_ResNet50 = model_ResNet50.evaluate_generator(test_generator)
print('Test loss:', score_ResNet50[0])
print('Test accuracy:', score_ResNet50[1])

"""# Recomendations :"""

from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions
import numpy as np
from scipy.spatial.distance import cosine

# Load the pre-trained ResNet50 model
model = model_ResNet50

# Load and preprocess the target image
target_image_path = 'fashion-dataset/images/10006.jpg'
img = image.load_img(target_image_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)

# Get the features of the target image
target_features = model.predict(img_array)

# Calculate the similarity between the target image and all other images in the dataset
def calculate_similarity(features1, features2):
    return 1 - cosine(features1, features2)

# Assuming you have a DataFrame called 'data' with a column 'filename' containing image paths
data['similarity_score'] = data['filename'].apply(lambda x: calculate_similarity(target_features, extract_features(x)))

# Sort the DataFrame based on similarity scores
recommended_images = data.sort_values(by='similarity_score', ascending=False)['filename'].tolist()

# Display or use the recommended images
print("Recommended images:")
for i, img_path in enumerate(recommended_images[:10]):  # Adjust the number of recommendations as needed
    print(f"{i + 1}. {img_path}")